##一些记录

数据方面：  
<br/>
　　1.每组数据包括full_train_set.csv和full_validate_set.csv。其目录名代表该组数据的构成方式。
如C1S4表示一支股票每条样本包含一个季度的特征和对应的标签，共通过滑动窗口生成4条样本，
C4S1则表示一支股票每条样本包含4个季度的特征和对应的标签，共通过滑动窗口生成1条样本。 
<br/>
　　2.验证数据比训练数据的最近特征时间再延后一个季度，并且每支股票只有一行。训练数据和验证数据的标签都是该样本中最近特征时间到下个季度该股票的超额收益是否超过沪深300指数的超额收益（表示该股票是否优于大盘）。 
<br/>
　　3.综上所述，当数据组成方式类似于CnSm（concern-n,slide-m）时，相当于对于每支股票，通过其最近的n个季度的数据来预测下个季度股票的表现是否优于大盘，并且通过在季度上向前滑动窗口m次，生成m条训练样本。 
<br/>
　　4.超额收益，策略收益和基准收益的含义：基准收益表示指数（如沪深300）在一定时间段内的收益百分比；策略收益表示一支股票在某一策略下（如“买入持有策略”）一段时间的收益比；超额收益表示一支股票超出基准收益的收益，朴素计算方法为“策略收益/基准收益”。 
<br/>
　　5.数据的生成顺序为：
<br/>
　　　　1.运行step1中的data_concat，生成仅包含研报数据的训练集和验证集。（二分类数据集）
<br/>
　　　　2.运行step2中的data_concat，生成带有行情序列的训练集和验证集。（二分类数据集）
<br/>
　　　　3.运行step2中的regression_data_producer，生成包含研报数据和行情序列的训练集和验证集。（回归数据集）
<br/>
　　　　4.通过step2中的分类和回归模型得到预测结果，再运行step3中的super_data_producer，生成super数据集和增量数据集，用来训练超模型。


模型方面：  
　　1.tf自带的特征列类，不能接受矩阵形式的输入，只能一列一列的绑定，否则喂数据的时候就会出现模型request一列，却feed了很多列。 
<br/>
　　2.跑模型时，出现“INFO:tensorflow:Done running local_init_op.”之后卡住很久不动，是因为特征列太多，正在加载。 
<br/>
　　超模型部分的执行顺序：
<br/>
　　　　1.在step2时，分类模型和回归模型对验证集（5个季度）的预测结果保存在了数据目录中。
<br/>
　　　　2.读取step2的预测结果，对数据进行拼接，生成“super数据”（共5份）。然后再对“super数据”进行增量拼接，比如，“增量数据1”仅包含“super数据1”中的所有样本；“增量数据2”则包含“super数据1”和“super数据2”中的所有样本。
<br/>
　　　　3.然后使用5份“增量数据”训练5个“超模型”，模型可以使用随机森林等。得到5个“超模型”之后，使用“超模型1”对“super数据2”进行打分和推荐；使用“超模型2”对“super数据3”进行打分和推荐等等。也就是说，“超模型5”和“super数据1”都是无用的。
<br/>
　　　　4.（可选方案）不生成增量数据，仅进行“错位”训练和验证

关于当前尝试的特征工程方法的一些思考：  
　　什么时候用归一化？什么时候用标准化？
<br/>
  （1）如果对输出结果范围有要求，用归一化。
<br/>
  （2）如果数据较为稳定，不存在极端的最大最小值，用归一化。
<br/>
  （3）如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。
<br/>
<br/>
<br/>

